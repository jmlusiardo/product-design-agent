# Define Product Assumptions

## Executive Summary

Defining assumptions is critical before building any product solution. The fastest 
way to ensure you're building the right solution is to prototype and test it with 
people before delivering to development. The Assumptions Test Board method provides 
a structured approach to identify, organize, and plan validation for all product 
assumptions your team is making.

Key outcomes: Clear list of testable assumptions, validation methods for each 
assumption, and success criteria to guide product decisions.

Time required: 30 minutes for initial session, ongoing collection throughout 
project lifecycle.

## Prerequisites

### Required Materials
- Whiteboard or digital collaboration tool (Miro, Figma, etc.)
- Markers for drawing table structure  
- Sticky notes or digital equivalents
- Team members from design, product, and development

### Required Knowledge
- Basic understanding of user research methods
- Familiarity with hypothesis testing concepts
- Knowledge of your product's target users and business goals

### When to Use This Method
- Always recommended: Keep tracking assumptions throughout product lifecycle
- Critical timing: During Converge phase of design sprints
- Best practice: Before any major feature development or product pivot
- Ongoing use: Add assumptions as they emerge during team discussions

## Quickstart

1. Set up the board - Draw 3-column table: Assumption | Test Method | Validated If
2. Collect assumptions - Have team write down assumptions on sticky notes
3. Organize assumptions - Place all assumptions in first column
4. Define tests - For each assumption, determine how you'll validate it
5. Set success criteria - Define what "validated" means with specific metrics

## Assumptions Test Board Template

### Setup Requirements
- Whiteboard or digital collaboration tool
- Sticky notes or digital equivalents  
- Markers for drawing table structure

### Table Structure

| Assumption                    | Test Method              | Validated If           |
|-------------------------------|--------------------------|------------------------|
| [What we believe to be true]  | [How we'll test it]      | [Success criteria]     |
|                               |                          |                        |
|                               |                          |                        |

### Instructions for Facilitators
1. Draw the 3-column table on whiteboard
2. Have team members write assumptions on sticky notes
3. Place assumptions in first column
4. For each assumption, define test method and success criteria
5. Prioritize assumptions by risk and testability

### Common Assumption Categories
- User Behavior: How users will interact with the product
- Market Demand: Whether people want this solution  
- Technical Feasibility: What's possible to build
- Business Model: How the product will generate value
- User Context: When/where/why users will use the product

## Procedure

### Step 1: Prepare the Assumptions Test Board

Create a dedicated space for collecting assumptions with the 3-column structure 
shown in the template above.

Physical setup: Use whiteboard with clear column divisions
Digital setup: Create shared board in Miro, Figma, or similar tool

### Step 2: Continuous Assumption Collection

Throughout your product development process:

- Listen actively during team meetings for statements starting with "users will...", 
  "people want...", "we think..."
- Write immediately - Capture assumptions on sticky notes as they emerge
- Encourage participation - Have all team members contribute assumptions
- No judgment - Collect all assumptions without filtering initially

### Step 3: Organize and Prioritize Assumptions

During your Converge session:

1. Group similar assumptions - Combine duplicates and related ideas
2. Prioritize by risk - Focus on assumptions that could kill the product if wrong
3. Consider testability - Prioritize assumptions you can realistically validate
4. Sequence testing - Order tests from quickest/cheapest to most complex

### Step 4: Define Validation Methods

For each assumption, determine the most appropriate test method:

Quick validation methods:
- User interviews (1-2 weeks)
- Online surveys (3-5 days)
- Analytics review (1-2 days)
- Competitive analysis (2-3 days)

Prototype-based methods:
- Usability testing with mockups (1-2 weeks)
- A/B testing with prototypes (2-4 weeks)
- Landing page tests (1-2 weeks)
- Wizard of Oz testing (1-3 weeks)

Longer-term validation:
- Beta testing programs (4-8 weeks)
- Market research studies (4-6 weeks)
- Technical spike/proof of concept (2-6 weeks)

### Step 5: Set Measurable Success Criteria

Define specific, measurable criteria for validation:

Good criteria examples:
- "75% of users choose option A over option B"
- "Average task completion time under 2 minutes"
- "Less than 20% of users express security concerns"
- "15% improvement in conversion rate"

Avoid vague criteria:
- "Users like it"
- "Positive feedback"
- "Better performance"
- "Good user experience"

## Examples

### Example 1: Mobile Banking App

| Assumption                           | Test Method                    | Validated If                    |
|--------------------------------------|--------------------------------|---------------------------------|
| Users prefer checking balance on     | Analytics review + user        | >60% of balance checks happen   |
| mobile vs desktop                    | interviews                     | on mobile devices               |
|--------------------------------------|--------------------------------|---------------------------------|
| Touch ID authentication is          | A/B test with interactive      | >75% of users choose Touch ID   |
| preferred over password              | prototype                      | when both options available     |
|--------------------------------------|--------------------------------|---------------------------------|
| Users trust mobile banking          | User interviews + security     | <20% express security concerns  |
| security                             | perception survey              | during interviews               |

### Example 2: E-learning Platform

| Assumption                           | Test Method                    | Validated If                    |
|--------------------------------------|--------------------------------|---------------------------------|
| Students prefer video lessons        | Prototype test with both       | Students spend >2x more time    |
| over text-based                      | content formats                | engaging with video content     |
|--------------------------------------|--------------------------------|---------------------------------|
| Progress tracking motivates          | A/B test with/without          | 15%+ improvement in course      |
| course completion                    | progress indicators            | completion rates                |
|--------------------------------------|--------------------------------|---------------------------------|
| Mobile learning happens primarily    | User diary study + contextual  | >50% of mobile learning         |
| during commutes                      | interviews                     | sessions occur during transit   |

## Troubleshooting

### Common Problems and Solutions

Problem: Team generates too many assumptions to test
Solution: Prioritize by potential impact and ease of testing. Focus on assumptions 
that could fundamentally change your product direction.

Problem: Assumptions are too vague to test effectively
Solution: Break broad assumptions into specific, testable components. "Users want 
better experience" becomes "Users want checkout in under 3 steps."

Problem: Success criteria are subjective
Solution: Convert qualitative goals to quantitative metrics. "Users find it 
intuitive" becomes "80% complete task without help."

Problem: Tests take too long or cost too much
Solution: Start with lightweight validation methods like surveys or interviews 
before investing in prototypes or development.

Problem: Team doesn't know how to test certain assumptions
Solution: Consult user research specialists or break complex assumptions into 
simpler, testable parts.

## FAQ

Q: How many assumptions should we test at once?
A: Start with 3-5 highest-risk assumptions. Testing too many simultaneously 
reduces focus and resource effectiveness.

Q: What if an assumption is partially validated?
A: Document the nuances. Partial validation often reveals important user segments 
or context dependencies worth exploring further.

Q: Should we test all assumptions before building anything?
A: Test the highest-risk assumptions first. Some assumptions can be validated 
during development or with minimal prototypes.

Q: How do we handle assumptions that can't be easily tested?
A: Break them into smaller, testable components or mark them as "monitor during 
development" if they're lower risk.

Q: What if testing invalidates a core product assumption?
A: This is valuable! Pivot your approach based on learnings rather than building 
something users don't want.

## Related Guides

- User Research Planning - Methods for testing assumptions with real users
- Prototype Testing - Validating assumptions through interactive prototypes  
- Design Sprint Facilitation - Running effective Converge sessions with assumption testing
- Hypothesis Formation - Writing testable product hypotheses from assumptions

## References
- Assumptions / Test Board - The Product Design Sprint: https://github.com/thoughtbot/design-sprint/blob/main/Exercises/assumptions.md