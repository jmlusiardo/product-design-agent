## Executive Summary

This guide provides comprehensive instructions for setting up effective usability tests in Userbrain.com, covering task types, screener questions, test configuration, and best practices. Userbrain is a remote usability testing platform that allows you to gather user feedback through video recordings of real users interacting with your product.

## Key Benefits:

Quick setup (tests can be configured in minutes)
Results typically delivered within hours
Access to 120k+ quality-assured testers worldwide
100% satisfaction guarantee on all tests
Pay-as-you-go pricing model

## Prerequisites & Assumptions

### Before You Begin:

- Userbrain account created (free trial available)
- Product/prototype accessible via URL (live websites, prototypes, competitor sites)
- Clear research objectives defined
- Target audience and user segments identified
- Budget allocated for participant incentives
- Team roles assigned (test creator, stakeholders, observers)

### Technical Requirements:

Internet-accessible URL for testing
Compatible with desktop, tablet, and mobile devices
Consider device-specific testing needs

## Quickstart: Creating Your First Test

### 5-Minute Setup Process

1. **Login to Dashboard**
   - Access your Userbrain dashboard
   - Click "Create New Test" in the top right corner

2. **Basic Test Configuration**
   - Enter test name and product URL
   - Choose target device type (Any, Desktop, Tablet, Mobile)
   - Set number of testers needed

3. **Add Tasks and Questions**
   - Use pre-built templates or create custom tasks
   - Mix different task types for comprehensive feedback
   - Keep tasks focused and actionable

4. **Configure Screening (Optional)**
   - Add up to 2 custom screener questions
   - Set demographic filters (country, age, gender)
   - Define pass/fail criteria for each question

5. **Review and Launch**
   - Preview your test setup
   - Verify all tasks and screening logic
   - Launch test and await results

## Task Types: Complete Reference

Userbrain offers 5 distinct task types, each serving different research objectives:

### 1. Task Completion
**Name:** Task  
**Description:** Users perform a specific action and click "Task completed" when finished  
**Best Used For:**
- Navigation and findability testing  
- Feature usage validation
- User flow completion analysis
- Task success rate measurement

**Example:**
"Find the pricing information for the Premium plan and add it to your cart."

### 2. Rating Scale
**Name:** Rating Scale / Star Rating  
**Description:** Users provide numerical ratings (typically 1-5 stars or custom scale)  
**Best Used For:**
- User satisfaction measurement
- Feature preference evaluation  
- Comparative assessment
- Overall experience rating

**Example:**
"Rate your overall satisfaction with the checkout process on a scale of 1-5 stars."
### 3. Multiple Choice
**Name:** Multiple Choice  
**Description:** Users can select more than one answer from provided options  
**Best Used For:**
- Feature preference identification
- Problem area discovery
- Multi-factor feedback collection
- User behavior analysis

**Example:**
"Which of the following features would you find most useful? (Select all that apply)
- [ ] Real-time notifications
- [ ] Advanced search filters  
- [ ] Export functionality
- [ ] Mobile app sync"

### 4. Single Choice
**Name:** Single Choice  
**Description:** Users can only select one option from the provided choices  
**Best Used For:**
- Decision point analysis
- Primary preference identification
- Demographic categorization
- Binary choice evaluation

**Example:**
"What is your primary reason for visiting this website?
○ Browse products
○ Make a purchase  
○ Get customer support
○ Learn about the company"

### 5. Written Response
**Name:** Written Response / Free-form  
**Description:** Users provide open-ended text responses  
**Best Used For:**
- Detailed feedback collection
- User mental model understanding
- Problem explanation and context
- Suggestion gathering

**Example:**
"Describe your overall impression of the website's design and what, if anything, you would change."

## Screener Questions: Filtering Your Audience

### Screener Question Types

#### 1. Single Select
**Description:** Testers can only select one option per question  
**Best Used For:**
- Demographic categorization (age groups, job roles)
- Experience level assessment
- Primary behavior identification
- Exclusive criteria filtering

**Validation Options:**
- **Must Select:** Required choice to pass screening
- **Must Not Select:** Automatic disqualification if selected  
- **Ignore:** Answer doesn't affect screening outcome

#### 2. Multiple Select  
**Description:** Testers can select one or more options per question  
**Best Used For:**
- Interest and hobby identification
- Multi-category behavior assessment
- Tool/platform usage patterns
- Comprehensive attribute filtering

**Important Note:** For Multiple Select questions marked as "Must Select," ALL selected options must be checked for the tester to pass.

### Built-in Demographic Filters

**Available Without Custom Questions:**
- **Device Type:** Any, Desktop, Tablet, Mobile
- **Country:** Target specific geographic regions
- **Age Range:** Define minimum and maximum age  
- **Gender:** Male, Female, Other, Prefer not to say

### Custom Screener Configuration

**Limitations:**
- Maximum 2 custom screener questions per test
- No conditional logic (questions cannot depend on previous answers)
- All screening happens before test participation

**Best Practices:**
- Make correct answers non-obvious to avoid gaming
- Include "None of the above" or "Other" options
- Use behavioral questions rather than direct product familiarity
- Test screener logic before launching

**Example Custom Screeners:**

**Single Select Example:**
"How often do you shop online?"
○ Daily
○ Weekly  
○ Monthly
○ Rarely (less than monthly)
○ Never

**Multiple Select Example:**  
"Which of the following devices do you use regularly? (Check all that apply)"
☐ Smartphone
☐ Laptop computer
☐ Desktop computer  
☐ Tablet
☐ Smart TV
☐ None of the above

## Test Setup Procedure

### Step 1: Project Configuration
- **Test Name:** Use descriptive, searchable names
- **Product URL:** Ensure accessibility and functionality
- **Test Type:** Standard remote unmoderated testing
- **Device Targeting:** Choose based on user research goals

### Step 2: Task Design Strategy

**Task Sequencing:**
1. Start with simple orientation tasks
2. Progress from basic to complex interactions  
3. End with open-ended feedback questions
4. Limit to 5-10 tasks total (avoid cognitive overload)

**Task Writing Guidelines:**
- Use imperative, action-oriented language
- Avoid leading or biased phrasing
- Provide realistic context and scenarios
- Keep instructions concise but clear
- Test tasks internally before launching

### Step 3: Screening Configuration

**Demographic Setup:**
1. Define target country/countries
2. Set age range parameters
3. Select gender preferences (if relevant)
4. Choose device requirements

**Custom Questions:**
1. Write behaviorally-focused questions
2. Set validation rules (Must Select/Must Not Select/Ignore)
3. Test logic flow manually
4. Include catch-all options

### Step 4: Quality Assurance

**Pre-Launch Checklist:**
- [ ] All URLs functional and accessible
- [ ] Task instructions clear and unbiased  
- [ ] Screener logic tested and validated
- [ ] Device compatibility verified
- [ ] Team stakeholder access configured
- [ ] Expected timeline communicated

## Advanced Configuration Options

### AI-Powered Task Creation
- **Auto-Create Feature:** Describe your product in natural language
- **AI Generation:** System creates tasks, questions, and instructions automatically  
- **Customization:** Edit AI-generated content to match specific needs
- **Best For:** Quick prototyping and overcoming "blank page syndrome"

### Integration Capabilities
- **User Interviews Integration:** Import recruited participants
- **TestingTime Integration:** Access niche demographic pools
- **Custom Participant Pools:** Invite your own users via shareable links
- **Stakeholder Collaboration:** Share results with team members

## Analysis & Reporting Features

### Automated Insights
- **AI Moment Marking:** Automatically identifies key moments in videos
- **Quantitative Metrics:** Task completion rates, time on task, average durations
- **Qualitative Analysis:** Organized participant feedback and observations
- **Presentation-Ready Reports:** Shareable with stakeholders

### Video Analysis Tools
- **Playback Controls:** Speed adjustment, rewind, fast-forward
- **Note-Taking:** Add timestamps and observations
- **Tester Ratings:** Rate participant quality (helps algorithm)
- **Transcript Access:** Searchable text of participant commentary
- **Full-Screen Mode:** Detailed interaction observation

## Best Practices & Guidelines

### Task Design Excellence
- **Professional Tone:** Clear, instructional, straightforward language
- **Realistic Scenarios:** Mirror actual user contexts and motivations
- **Unbiased Phrasing:** Avoid leading participants toward specific outcomes
- **Actionable Instructions:** Enable participants to complete tasks successfully

### Screening Optimization  
- **Behavioral Focus:** Ask about actions, not attitudes or intentions
- **Avoid Gaming:** Make desired answers non-obvious to prevent manipulation
- **Include Validation:** Add verification tasks within the test itself
- **Test Screening Logic:** Manually verify question flow before launch

### Quality Assurance
- **Pilot Testing:** Run internal tests before live deployment
- **Stakeholder Alignment:** Confirm research objectives with team
- **Device Testing:** Verify functionality across target platforms  
- **Results Review:** Analyze participant quality and adjust future screening

## Troubleshooting Common Issues

### Low Response Rates
**Symptoms:** Insufficient participants or slow recruitment  
**Solutions:**
- Broaden demographic criteria
- Simplify screening requirements
- Check URL accessibility
- Verify competitive compensation

### Poor Quality Responses  
**Symptoms:** Participants not following instructions or providing shallow feedback
**Solutions:**
- Revise task instructions for clarity
- Add verification questions
- Implement stricter screening criteria  
- Use tester rating system for quality feedback

### Technical Access Problems
**Symptoms:** Participants cannot access product or complete tasks
**Solutions:**
- Test all URLs in different browsers/devices
- Ensure no login requirements for testers
- Check for geographical restrictions
- Provide alternative access methods

### Screening Logic Errors
**Symptoms:** Wrong participants passing through or qualified users being rejected
**Solutions:**  
- Review Must Select/Must Not Select settings
- Test screening combinations manually
- Simplify complex screening requirements
- Add catch-all "Other" options

## Templates & Examples

### E-commerce Website Test Template
Task 1 (Task): Find a product under $50 in the Electronics category
Task 2 (Single Choice): What was your first impression of the homepage?
Task 3 (Task): Add the product to your cart and begin checkout
Task 4 (Rating): Rate the checkout process (1-5 stars)
Task 5 (Written Response): What would prevent you from completing this purchase?

### Mobile App Prototype Test Template
Task 1 (Task): Create a new account using the sign-up process
Task 2 (Multiple Choice): Which features seem most useful? (Select all)
Task 3 (Task): Complete your profile setup
Task 4 (Rating): How intuitive was the navigation? (1-5 stars)
Task 5 (Written Response): Describe any confusing or frustrating moments

### Screener Question Examples
Demographics: Age 25-45, Any Country, Any Gender, Desktop/Mobile
Custom Question 1 (Single Select): "How often do you use mobile apps for shopping?"
Daily → Must Select
Weekly → Must Select
Monthly → Ignore
Rarely → Must Not Select
Never → Must Not Select

Custom Question 2 (Multiple Select): "Which platforms do you use regularly?"
Amazon → Must Select (at least one)
eBay → Must Select (at least one)
Local stores only → Must Not Select
I don't shop online → Must Not Select

## Glossary

**AI Moment Marking:** Automated identification of significant user interactions or feedback
**Must Not Select:** Screening validation that disqualifies participants who choose this option  
**Must Select:** Screening validation requiring this choice for participation
**Screener:** Set of questions used to filter and qualify test participants
**Task Completion Rate:** Percentage of participants who successfully complete a given task
**Tester Pool:** Database of available participants for usability testing
**Time on Task:** Duration measurement for task completion analysis
**Unmoderated Testing:** Testing conducted without live facilitator presence

## Cross-References

### Related Task Guides

**For Comprehensive Research Planning:**
- `test_plan.md` — Complete framework for creating usability test plans with intake forms, task structuring, and success criteria definition. Essential for planning complex studies that extend beyond Userbrain's scope or require detailed stakeholder alignment and custom metrics tracking.

**For Participant Recruitment Strategy:**  
- `recruiting_users.md` — In-depth participant sourcing, screening design, and recruitment best practices. Use when Userbrain's built-in tester pool doesn't meet your specific demographic needs or when you need to recruit niche user segments requiring custom screening approaches.

**For Broader Testing Methodology:**
- `usability_testing.md` — End-to-end usability testing process covering formative vs. summative studies, facilitation techniques, and pre/post-test questionnaires. Reference for understanding testing fundamentals and when to choose different research approaches beyond Userbrain's unmoderated format.

**For Expert-Based Evaluation:**
- `ux-audit-expert-review.md` — Heuristic evaluation and expert review processes for systematic usability assessment. Use as a complement to Userbrain testing when you need structured evaluation criteria or want to validate findings with expert analysis before user testing.

**For Evaluation Method Selection:**
- `evaluation_type.md` — Framework for choosing appropriate research methods (formative, summative, investigative). Consult when deciding whether Userbrain's approach fits your research objectives or if alternative methods would be more suitable.

### Supporting Materials

- **Question Templates:** `user_feedback_questions.md` - Comprehensive repository of tested survey questions organized by category and use case
- **Pre/Post-Test Questions:** Reference for rating scale questions, satisfaction measures, and follow-up probes to enhance Userbrain task scenarios
- **Research Enhancement:**
   - `cognitive_biases.md` — Understanding biases that affect user testing and screening. Critical for designing unbiased Userbrain tasks and interpreting results objectively.
   - `prompt_templates_registry.md` — AI prompt templates for research tasks including learning facilitation and problem-solving. Useful for generating task scenarios or analyzing Userbrain results with AI assistance.
   - `product_metrics_list.csv` — Comprehensive metrics catalog for defining measurable success criteria. Reference when establishing quantitative benchmarks for Userbrain task success rates and user satisfaction scores. d

## References
- Userbrain Help Documentation: https://help.userbrain.com/help  
- Userbrain Screening Questions Guide: https://help.userbrain.com/help/use-screening-questions-to-filter-testers  
- Userbrain Blog - User Testing Best Practices: https://www.userbrain.com/blog/user-testing-best-practices/  
- Userbrain AI Task Creation: https://www.userbrain.com/blog/auto-create-tasks-and-questions-for-your-user-tests  
- Filter Testers Using Screening Questions: https://www.userbrain.com/blog/new-feature-filter-testers-screening-questions