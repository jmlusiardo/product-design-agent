# Creating Usability Testing Test Plans: The Complete Guide

This guide shows exactly how to **create a complete, consistent usability test plan**—from kickoff inputs to a ready-to-run plan with tasks, metrics, roles, and materials. It is **focused only on plan creation** (separate guides will cover facilitation, analysis, and reporting).  

---

## Scope & Audience

- **Who:** UX Researchers, PMs, Designers, and stakeholders drafting moderated/remote usability test plans.  
- **When:** Before scheduling sessions; after aligning test goals and scope.  
- **Out of scope:** Live moderation tactics, analysis frameworks, and report writing (handled in separate guides).  

---

## Prerequisites

- **Problem framing & goals** (what evidence you need).  
- **Prototype or product access** (lo-fi or hi-fi) and test data/assets.  
- **Recruiting criteria** (segments; # per segment).  
- **Consent & recording approach** (privacy notice, storage).  
- **Pre-/Post-test questionnaires** ready to link or attach.  

---

## Quickstart (Five Decisions)

Make these five choices before drafting:

1. **Study type:** *Formative* (few users; quick insights) vs *Summative* (more users; metrics focus).  
2. **Fidelity:** *Lo-fi* (wireframes) vs *Hi-fi* (polished UI/prod).  
3. **Session length & count:** Plan **60–90 min** per session (max **120 min**).  
4. **Tasks list:** Aim for **~10–15 tasks**, ordered from easy ➜ hard, with an orientation task first.  
5. **Metrics to capture:** Success/Failure, Time on Task, Severity/Effort, plus qualitative notes.  

---

## Procedure / SOP — Create the Test Plan

### Step 1 — Kickoff & Inputs
- Align on **business goals, user objectives, and decisions** the study must inform.  
- Inventory **functional scope** you can test now (prototype or product constraints).  
- Confirm **segments & sample size** (e.g., 3–5 per segment for agile/weekly; 8–15 for formative; 20–50+ for summative).  

### Step 2 — Define What You’re Testing
- Write a crisp **“What we’re testing”** statement (feature(s), flows).  
- Capture **“Test goals”**: what to validate, compare, or uncover. *(Mirror the simple headings used in prior plans.)*  

### Step 3 — Draft Tasks (Scenarios)
- Start with an **orientation task**, then escalate difficulty.  
- Use open prompts first (“How would you…?”) to observe the **intuitive path**; add specificity to probe edge cases.  
- End each task with a **question**; state a clear **end state** (answer) and **success criteria**.  

### Step 4 — Choose Metrics & Evidence
- Quantitative: **Success/Failure/Partial, Time on Task, Effort (clicks/progress perception), Severity**.  
- Qualitative: **Satisfaction, perceived difficulty, stress/body-language cues**.  

### Step 5 — Roles, Agenda, and Materials
- Define **roles** (Facilitator, Moderator, PM, Note-takers, Observers) and **who may probe**.  
- Create a **tight agenda** (intro → pre-test → tasks → post-test → Q&A).  
- List **materials/links**: prototype start state, pre/post forms, consent text, observer note sheet.  

### Step 6 — Scheduling & Logistics
- Build a **calendar table** (Participant, Company, Date/Time, Recording link, Status).  
- Confirm **recording & consent** plan and storage.  

### Step 7 — Finalize & Pilot
- **Pilot** the plan (internal or with 1 user) to validate tasks, timing, and links; adjust before full run.  

---

## Decision Points (Plan Options Matrix)

| Decision | Option A | Option B | Use When… |
|---|---|---|---|
| Study type | Formative | Summative | Need quick directional insight vs. need robust metrics across many users. |
| Fidelity | Lo-fi | Hi-fi | Early concept risk reduction vs. mature UI polish checks. |
| Session time | 60–90 min | up to 120 min | Standard moderated vs. exceptionally complex flows. |
| Task count | ~10–15 | Fewer/More | Balance breadth with depth and participant fatigue. |
| Metrics | Core (Success, Time, Severity) | Add-ons (effort clicks, perceived progress) | When influencing skeptical stakeholders with numbers. |

flowchart LR
K[Kickoff & Inputs] --> S[Study Type & Fidelity]
S --> T[Tasks & Success Criteria]
T --> M[Metrics & Evidence]
M --> R[Roles, Agenda, Materials]
R --> Sch[Schedule & Consent]
Sch --> P[Pilot & Finalize]

## Templates & Snippets

### A. Task Writing Patterns
- **Open route:** “How would you *[goal]*?” → observe path.  
- **Context cue:** “You *[motivation]*… What is…/Which…/How can…?”  
- **Specific check:** “According to *[app]*, what…?”  
- **Constraints:** If needed, state “Use *[this product/prototype]*.”  

### B. Post-Test Question Ideas
- “If you could **change one thing**, what would it be?”  
- “If you could **keep only one thing**, what would it be?”  

### C. Questionnaire Links (Examples)
- **Pre-Test Questionnaire (example)** — brand familiarity, usage context, comfort prompts.  
- **Post-Test Questionnaire (example)** — follow-ups + task difficulty ratings.  

---

## Intake Form (Copy-Paste)

> **Use this first.** Fill to auto-populate the plan.

### Test Plan Intake

- Product/Feature Name:
- What We’re Testing (scope/features/flows):
- Why Now (decisions to inform):
- Study Type: [Formative | Summative]
- Fidelity: [Lo-fi | Hi-fi]  Prototype/Build Link:
- Segments & Sample: (e.g., 3–5 per segment; total N= )
- Session Length: [60 | 75 | 90] mins (Max 120)  Timebox per Task:
- Tasks Target Count: ~10–15  Orientation Task: [Yes/No]
- Core Metrics: [Success/Fail/Partial, Time on Task, Severity, Effort]
- Qual Metrics: [Satisfaction, perceived difficulty, notable quotes]
- Constraints/Assumptions:
- Risks to Validity (and mitigations):
- Recording & Consent: [On/Off] Storage:
- Pre-Test Questionnaire Link:
- Post-Test Questionnaire Link:
- Stakeholders & Decisions (who/what):

## Usability Test Plan Template (Copy-Paste)

# [PROJECT/FEATURE] — Usability Testing — Test Plan
## What We’re Testing
Brief description of the feature(s)/flows and prototype/build links.

## Test Goals
- Goal 1
- Goal 2
- Goal 3

## Related Tickets / Docs
- [ID] Title / Link
- [ID] Title / Link

## Participants & Segments
- Segment A: N=
- Segment B: N=
Recruiting notes & must-have traits.

## About the Test
- Format: [Remote | In-person], [Moderated]
- Fidelity: [Lo-fi | Hi-fi]
- Session Length: [60–90] min (≤120)
- Environment/Devices:
- Recording & Consent: [Yes/No]  Storage/Access:

## Roles
- Facilitator:
- Moderator:
- PM (probing allowed?): [Yes/No]
- Note Taker(s):
- Observers (Q&A only):

## Agenda (Timeboxed)
1. Intro & consent — [5 min]
2. Pre-test questionnaire — [10 min]
3. Tasks — [X min]
4. Post-test questionnaire — [5 min]
5. Q&A — [remaining]

## Materials & Links
- Prototype start state(s):
- Test data / credentials:
- Pre-test questionnaire:
- Post-test questionnaire:
- Observer note sheet:
- Severity rubric:
- Any special instructions:

## Calendar / Scheduling
| Date | Time (TZ) | Company | Participant | Segment | Moderator | Recording Link | Status |
|---|---|---|---|---|---|---|---|

## Tasks
> Run from easy ➜ hard; start with an orientation task.

### Task 1 — [Title]
- **Scenario:** “How would you …?”
- **End State:** What counts as done/found.
- **Success Criteria:** [Success | Partial | Fail] definitions.
- **Follow-ups:** “Could you get this elsewhere?” “What would you do next?” 
- **Metrics:** Success/Fail/Partial; Time on Task; Effort; Severity.

### Task 2 — [Title]
- ...

## Metrics to Capture (Per Task)
- **Quant:** Success/Fail/Partial; Time on Task; Effort; Severity.
- **Qual:** Satisfaction (verbal), perceived difficulty, notable quotes, stress/affect.
- **Notes:** Observed confusions/errors/highlights.

## Risk & Assumptions
- [Assumption or constraint] → [Mitigation]

## Appendices
- Consent text
- Glossary

## Task Sheet Format for Usability Testing

Each usability test **task** must be documented in a structured **task sheet** table.  
This ensures consistency across sessions and provides clear traceability to **design KPIs** and later **reporting** steps.  

---

### Column Definitions

1. **Behavior** — description of the user behavior being tested.  
2. **Question** — moderator’s prompt to trigger that behavior.  
3. **Converts if they… (Success Criteria)** — what counts as success, aligned with `design_kpis.md`.  
4. **Efficacy** — to be filled during test, using only:  
   - Converts  
   - Doesn’t Convert  
   - Converts with Doubts  
   - Doesn’t Apply  
5. **Observations** — moderator notes, details, and verbatims.  

---

### Task Sheet Template

| # | Behavior | Question | Converts if they... | Efficacy | Observations |
|---|----------|----------|----------------------|----------|--------------|
| 1 | [User behavior] | [Moderator question] | [Success criteria, aligned with KPIs] | [Converts / Doesn’t Convert / Converts with Doubts / Doesn’t Apply] | [Moderator notes + verbatims] |
| 2 | ... | ... | ... | ... | ... |

| # | Behavior | Question | Converts if they... | Efficacy | Observations |
|---|----------|----------|----------------------|----------|--------------|
| 1 | User mental model for job of ensuring quality of answers | What do you think you might need to do in order to ensure the quality of this agent answers? | Say they need to review or audit answers first, then maybe modify them. | Converts | "I would expect is to go either in analytics or integration… search for the product and see what comes up and how it looks." |
| 2 | Merchant navigation intuition / mental model | Where would you go in this dashboard if you want to review a product questions? | Locate Questions Workspace without more than 3 navigation errors or confusion. | Doesn’t Convert | Tried Analytics, Searchandising, Integrations. After hint, found ‘Questions’ but said label unclear; “Workspace” was confusing. |

## Examples & Snippets (Drop-In)

- **Success rubric (example):**  
  - *Success:* Reaches end state without moderator help.  
  - *Partial:* Reaches with hints or workaround.  
  - *Fail:* Cannot reach; abandons after timebox.  

- **Effort note (example):** “User needed 7 clicks; progress felt unclear after step 3.”  

- **Calendar row (example):**  
  `2025-07-18 | 10:00 CET | ACME | P-03 | Seg A | J.D. | [link] | Done`  


---

## Glossary

- **Formative testing:** Few users; quick insight for iteration. **Summative:** Larger N; stronger metric focus.  
- **Lo-fi / Hi-fi:** Prototype fidelity used in the session.  
- **Severity:** Combined **impact** and **% affected** measure for issues.  

---

## Cross-References
- *design_kpis.md* — Provides the **measurable criteria** (e.g., time on task thresholds, navigation error limits, effort metrics) used when defining “Converts if they…” success criteria in the **Task Sheet**. Ensures each task links back to objective KPIs.  
- *reporting_test_results.md* — Explains how to **aggregate and report outcomes** after usability testing. Cross-referenced from the **Task Sheet** and **Procedure** sections to clarify the next steps once success/failure criteria and efficacy are logged.  
- *For validated question templates across all research phases, reference `user_feedback_questions.md` in the materials directory.*

---

## References
- Checklist for Planning Usability Studies, by Hoa Loranger: https://www.nngroup.com/articles/usability-test-checklist/
- A Beginner's Guide to Usability testing, Maze: https://maze.co/guides/usability-testing/
- The 1-page usability testing plan, by David Travis: https://medium.com/@userfocus/the-1-page-usability-test-plan-dbc8c3d7fb54
- 11 steps for creating a usability testing plan that works, Dovetail: https://dovetail.com/ux/steps-to-create-a-usability-testing-plan/