# Distinguishing Data, Information, and Strategic Knowledge

> **Executive Summary**  
> This guide provides a structured framework for differentiating raw data, processed information, and strategic knowledge in UX decision-making. It highlights common pitfalls, practical applications, and training approaches for teams. The goal is to align terminology, improve research rigor, and enable better design and business outcomes.

## Overview
- **Purpose:** Clarify distinctions between data, information, knowledge, findings, and insights.  
- **Scope:** UX research, team alignment, design sprints, and stakeholder communication.  
- **Audience:** UX teams, researchers, product managers, marketing teams, junior designers.  
- **Success Criteria:** Teams consistently apply correct terminology, avoid premature conclusions, and base strategic decisions on validated insights.

## Preparation
- **Research & Planning**
  - Define shared terminology before projects begin.
  - Collect relevant data sources with attention to quality.
  - Train junior team members on research fundamentals.
- **Checklist**
  - [ ] Agree on definitions of data, information, and knowledge.
  - [ ] Identify potential misuse of “insight” in team communication.
  - [ ] Prepare case examples to demonstrate distinctions.
  - [ ] Validate participant recruitment criteria.
  - [ ] Assign a facilitator to enforce methodological rigor.

## Main Flow / Process

### 1) Raw Data
- **Definition:** Objective facts without interpretation.  
- **Examples:**
  - 200 button clicks
  - 35% Android sessions
  - 48-second checkout time  
- **Limitations:** Necessary as input but insufficient for decision-making.

### 2) Processed Information
- **Definition:** Data analyzed with context to show patterns or trends.  
- **Example:** 65% clicks go to register vs 35% to explore content.  
- **Limitations:** Reveals “what” but not “why.”

### 3) Strategic Knowledge
- **Definition:** Information connected with broader context and expertise.  
- **Function:** Explains root causes, enables strategic decision-making.  
- **Example:** Linking usage decline to demographic preference for in-person banking.  

### 4) Application Case: Itaú
- **Data:** 26% of users not using Itaú Pagos.  
- **Premature Reaction:** Jumped to solution-building.  
- **Additional Information:** Affected users were 70+ years old.  
- **Knowledge Outcome:** No product change required—demographic aligned with offline behavior.  
- **Impact:** Saved months of unnecessary development.  

### 5) Findings vs Insights
- **Finding:** Observable fact from research.
  - Example: 60% feel insecure about international payments.  
- **Insight:** Deep causal understanding explaining motivations.
  - Example: Insecurity stems from unclear display of final amounts in origin currency.  
- **Rule:** All projects yield findings; insights require rigor and cannot be guaranteed.

## Templates / Canvases
- **Terminology Alignment Canvas**
  - Columns: *Data*, *Information*, *Knowledge*, *Findings*, *Insights*.
  - Exercise: Sort examples into correct categories as a team.
- **Case Reflection Template**
  - *Raw data observed*
  - *Processed information extracted*
  - *Knowledge conclusion and decision impact*
- **Insight Validation Checklist**
  - [ ] Supported by multiple data points
  - [ ] Explains “why” behind observed behavior
  - [ ] Tested against alternative explanations
  - [ ] Linked to actionable strategy

## Aftermath / Follow-ups
- Summarize project outcomes with explicit labeling of data vs information vs knowledge.  
- Share learning materials with team (e.g., Itaú case).  
- Schedule periodic terminology refresh workshops.  
- Document validated insights in a shared repository.  

## Best Practices & Pitfalls
- **Do**
  - Use raw data as building blocks, not final answers.
  - Challenge premature solutioning from single data points.
  - Distinguish between findings and insights clearly.
  - Encourage critical thinking among junior researchers.  
- **Avoid**
  - Labeling every observation as an “insight.”
  - Letting decision-makers override evidence with personal preferences.
  - Recruiting participants without proper screening.
  - Relying on poor-quality data sources.  

## Tools & Resources
- **Digital experimentation:** Low-cost A/B testing to validate hypotheses quickly.  
- **Research training programs:** e.g., Hello Hello’s 4–6 session format for junior designers.  
- **Design sprints:** Adapted 2-week formats for smoother integration and better data quality control.  